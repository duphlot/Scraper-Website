{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAHASA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khởi Tạo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4 numpy pandas requests selenium matplotlib seaborn regex geckodriver-autoinstaller webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import urllib.request as urllib2\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from selenium.webdriver.common.by import By\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import argparse\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import os\n",
    "import regex as re\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException, ElementNotVisibleException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import geckodriver_autoinstaller\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = ['https://www.fahasa.com/sach-trong-nuoc/van-hoc-trong-nuoc/tieu-thuyet.html'] + \\\n",
    "    [f'https://www.fahasa.com/sach-trong-nuoc/van-hoc-trong-nuoc/tieu-thuyet.html?order=num_orders&limit=24&p={i}' for i in range(2, 87)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookTitles = []\n",
    "images = []\n",
    "urls = []\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\"}\n",
    "\n",
    "def take_bookTitles_and_images(page):\n",
    "    r = requests.get(page, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    for img_tag in soup.find_all('span', class_='product-image'):\n",
    "        bookTitles.append(img_tag.find('img')['alt'])\n",
    "        images.append(img_tag.find('img')['data-src'])\n",
    "    \n",
    "    urls.extend([a_tag['href'] for a_tag in soup.find_all('a', class_='product-image')])\n",
    "\n",
    "for page in pages:\n",
    "    take_bookTitles_and_images(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "wantedBooks = len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = []\n",
    "details = []\n",
    "genres = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_or_default(soup, selector, default=\"No detail\"):\n",
    "    element = soup.select_one(selector)\n",
    "    return element.text if element else default\n",
    "\n",
    "def extract_genres(soup):\n",
    "    genresRaw = soup.select('ol.breadcrumb a')\n",
    "    return [genre.text for genre in genresRaw] if genresRaw else [\"No genre\"]\n",
    "\n",
    "def take_prices_inf_genre(url):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    price = extract_text_or_default(soup, 'span.price', \"No price\")\n",
    "    detail = extract_text_or_default(soup, 'div.std', \"No detail\")\n",
    "    genre = extract_genres(soup)\n",
    "    \n",
    "    prices.append(price)\n",
    "    details.append(detail)\n",
    "    genres.append(genre)\n",
    "\n",
    "for url in urls:\n",
    "    take_prices_inf_genre(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bookTitles))\n",
    "print(len(prices))\n",
    "print(len(details))\n",
    "print(len(images))\n",
    "print(len(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Ten \": bookTitles,\"Gia \": prices ,\"Thong tin\": details,\"Anh\":images,\"The loai\":genres}).to_csv(\"data_fahasa.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the data from the csv file\n",
    "data = pd.read_csv(\"data_fahasa.csv\")\n",
    "data = data.drop(columns=[\"Unnamed: 0\"])\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookTitles = data[\"Ten \"]\n",
    "prices = data[\"Gia \"]\n",
    "details = data[\"Thong tin\"]\n",
    "images = data[\"Anh\"]\n",
    "genres = data[\"The loai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookTitlesWithoutToneMark = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_accent_vietnamese(s):\n",
    "    s = re.sub('[áàảãạăắằẳẵặâấầẩẫậ]', 'a', s)\n",
    "    s = re.sub('[ÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬ]', 'A', s)\n",
    "    s = re.sub('[éèẻẽẹêếềểễệ]', 'e', s)\n",
    "    s = re.sub('[ÉÈẺẼẸÊẾỀỂỄỆ]', 'E', s)\n",
    "    s = re.sub('[óòỏõọôốồổỗộơớờởỡợ]', 'o', s)\n",
    "    s = re.sub('[ÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢ]', 'O', s)\n",
    "    s = re.sub('[íìỉĩị]', 'i', s)\n",
    "    s = re.sub('[ÍÌỈĨỊ]', 'I', s)\n",
    "    s = re.sub('[úùủũụưứừửữự]', 'u', s)\n",
    "    s = re.sub('[ÚÙỦŨỤƯỨỪỬỮỰ]', 'U', s)\n",
    "    s = re.sub('[ýỳỷỹỵ]', 'y', s)\n",
    "    s = re.sub('[ÝỲỶỸỴ]', 'Y', s)\n",
    "    s = re.sub('đ', 'd', s)\n",
    "    s = re.sub('Đ', 'D', s)\n",
    "    return s\n",
    "\n",
    "def separate(title):\n",
    "    return title.split('(')[0].split('-')[0]\n",
    "\n",
    "goodReadsOriginalLink = 'https://www.goodreads.com/'\n",
    "goodReadsOriginalSearchLink = 'https://www.goodreads.com/search?q='\n",
    "goodReadsLinkList = []\n",
    "\n",
    "bookTitlesWithoutToneMark = [separate(bookTitle) for bookTitle in bookTitles]\n",
    "\n",
    "for bookTitleWithoutToneMark in bookTitlesWithoutToneMark:\n",
    "    urlForBookTitleInGoodReads = goodReadsOriginalSearchLink + '%20'.join(bookTitleWithoutToneMark.split())\n",
    "    urlForBookTitleInGoodReads = no_accent_vietnamese(urlForBookTitleInGoodReads)\n",
    "    try:\n",
    "        html = urlopen(urlForBookTitleInGoodReads)\n",
    "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "        bookTitle = bs.find('a', class_='bookTitle')\n",
    "        goodReadsLinkList.append(goodReadsOriginalLink + bookTitle['href'])\n",
    "    except Exception as e:\n",
    "        goodReadsLinkList.append('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(goodReadsLinkList))\n",
    "print(goodReadsLinkList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "inf = []\n",
    "rating = []\n",
    "all_genres = []\n",
    "\n",
    "def extract_data(soup, selector, attr=None, default=\"\"):\n",
    "    element = soup.select_one(selector)\n",
    "    if element:\n",
    "        return element[attr] if attr else element.text.strip()\n",
    "    return default\n",
    "\n",
    "def genre_about(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.138 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        rate_dict = {'5': 0, '4': 0, '3': 0, '2': 0, '1': 0}\n",
    "        json_data = soup.find('script', type='application/ld+json')\n",
    "        if json_data:\n",
    "            data = json.loads(json_data.string)\n",
    "            if \"aggregateRating\" in data:\n",
    "                rate_dict = data['aggregateRating']['ratingValue']\n",
    "\n",
    "        desc_text = extract_data(soup, 'div.DetailsLayoutRightParagraph__widthConstrained')\n",
    "\n",
    "        genres = [genre_link.text.strip() for genre_link in soup.select('div[data-testid=\"genresList\"] a')]\n",
    "        genres = list(set(genres)) if genres else []\n",
    "\n",
    "        inf.append(desc_text)\n",
    "        rating.append(rate_dict)\n",
    "        all_genres.append(genres)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        inf.append('')\n",
    "        rating.append('')\n",
    "        all_genres.append('')\n",
    "\n",
    "# Thực thi và in kết quả\n",
    "for goodReadsLink in goodReadsLinkList:\n",
    "    genre_about(goodReadsLink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_rating_ratingBook = [{}]\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.headless = True \n",
    "def main(url,book): \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.138 Safari/537.36'\n",
    "    }\n",
    "    retries = 3\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            scriptData = soup.find('script', id=\"__NEXT_DATA__\")\n",
    "            for i in json.loads(scriptData.string)['props']['pageProps']['apolloState']:\n",
    "                if (json.loads(scriptData.string)['props']['pageProps']['apolloState'][i]['__typename'] == 'Review'):\n",
    "                    temp = json.loads(scriptData.string)['props']['pageProps']['apolloState'][i]\n",
    "                    dict_id_rating_ratingBook.append({'id':temp['creator']['__ref'].replace('User:', ''),'bookName':book,'rating':temp['rating'],'ratingBook':temp['text']})\n",
    "            break\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTPError: {e}\")\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "for i in range(len(goodReadsLinkList)):\n",
    "    main(goodReadsLinkList[i],bookTitles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = []\n",
    "user_rating = []\n",
    "user_rating_book = []\n",
    "user_review = []\n",
    "for i in range(1,len(dict_id_rating_ratingBook)):\n",
    "    user_id.append(dict_id_rating_ratingBook[i]['id'])\n",
    "    user_rating.append(dict_id_rating_ratingBook[i]['rating'])\n",
    "    user_rating_book.append(dict_id_rating_ratingBook[i]['bookName'])\n",
    "    user_review.append(dict_id_rating_ratingBook[i]['ratingBook'])\n",
    "print(len(user_id))\n",
    "print(len(user_rating))\n",
    "print(len(user_rating_book))\n",
    "print(len(user_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"id \": user_id,\"rating \": user_rating ,\"name\": user_rating_book,\"Review\": user_review}).to_csv(\"review_data.csv\")\n",
    "pd.DataFrame({'thong tin':inf,'rating':rating,'genre':all_genres}).to_csv(\"good_read.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gộp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhs = pd.read_csv('data_fahasa.csv')\n",
    "gr = pd.read_csv('good_read.csv')\n",
    "rd = pd.read_csv('review_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_accent_vietnamese(s):\n",
    "    s = re.sub('\\xa0', ' ', s)\n",
    "    s = re.sub('[áàảãạăắằẳẵặâấầẩẫậ]', 'a', s)\n",
    "    s = re.sub('[ÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬ]', 'A', s)\n",
    "    s = re.sub('[éèẻẽẹêếềểễệ]', 'e', s)\n",
    "    s = re.sub('[ÉÈẺẼẸÊẾỀỂỄỆ]', 'E', s)\n",
    "    s = re.sub('[óòỏõọôốồổỗộơớờởỡợ]', 'o', s)\n",
    "    s = re.sub('[ÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢ]', 'O', s)\n",
    "    s = re.sub('[íìỉĩị]', 'i', s)\n",
    "    s = re.sub('[ÍÌỈĨỊ]', 'I', s)\n",
    "    s = re.sub('[úùủũụưứừửữự]', 'u', s)\n",
    "    s = re.sub('[ÚÙỦŨỤƯỨỪỬỮỰ]', 'U', s)\n",
    "    s = re.sub('[ýỳỷỹỵ]', 'y', s)\n",
    "    s = re.sub('[ÝỲỶỸỴ]', 'Y', s)\n",
    "    s = re.sub('đ', 'd', s)\n",
    "    s = re.sub('Đ', 'D', s)\n",
    "    return s\n",
    "\n",
    "def twodtooned(m):\n",
    "    return [no_accent_vietnamese(lt) if isinstance(lt, str) else '' for lt in m]\n",
    "\n",
    "list_name = twodtooned(fhs['Ten '])\n",
    "temp = twodtooned(rd['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inf = []\n",
    "list_genres = []\n",
    "real_name = []\n",
    "rdid = []\n",
    "rdrating = []\n",
    "list_img = []\n",
    "\n",
    "def find_index(theList, item):\n",
    "    for ind, val in enumerate(theList):\n",
    "        if item in val:\n",
    "            return ind\n",
    "    return 'Not Found'\n",
    "\n",
    "def append_data(res, i):\n",
    "    if res != 'Not Found':\n",
    "        list_img.append(fhs['Anh'][res])\n",
    "        list_inf.append(fhs['Thong tin'][res])\n",
    "        list_genres.append(gr['genre'][res])\n",
    "        real_name.append(fhs['Ten '][res])\n",
    "    else:\n",
    "        list_inf.append('')\n",
    "        list_genres.append('')\n",
    "        list_img.append('')\n",
    "        real_name.append('')\n",
    "\n",
    "for i, item in enumerate(temp):\n",
    "    res = find_index(list_name, item)\n",
    "    rdid.append(rd['id '][i])\n",
    "    rdrating.append(rd['rating '][i])\n",
    "    append_data(res, i)\n",
    "\n",
    "temple = pd.read_csv(\"something.csv\")\n",
    "st = temple['rating-user']\n",
    "for i in range(len(st)):\n",
    "    if isinstance(st[i], float):\n",
    "        list_img.append(fhs['Anh'][i])\n",
    "        real_name.append(temple['Ten'][i])\n",
    "        list_inf.append(temple['Thong tin'][i])\n",
    "        list_genres.append(temple['The loai'][i])\n",
    "        rdid.append('')\n",
    "        rdrating.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'id':rdid,'rating':rdrating,'name':real_name,'Hinh':list_img,'Thong tin':list_inf,'The Loai':list_genres}).to_csv('All.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileCSV = pd.read_csv(\"All.csv\")\n",
    "pd.DataFrame({'User-ID':fileCSV['id']}).to_csv('Users.csv')\n",
    "pd.DataFrame({'User-ID':fileCSV['id'],'Book-Rating':fileCSV['rating']}).to_csv('Ratings.csv')\n",
    "pd.DataFrame({'Book-Title':fileCSV['name'],'Image':fileCSV['Hinh'],'Description':fileCSV['Thong tin'],'Genres':fileCSV['The Loai']}).to_csv('Books.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
