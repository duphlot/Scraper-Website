{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uetCMyRhEkvJ"
      },
      "source": [
        "# FAHASA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AL0fLReEkvK"
      },
      "source": [
        "## Khởi Tạo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4ruqpXYEkvL",
        "outputId": "bc251782-a66e-4e62-d98d-6617b5c45483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.28.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: geckodriver-autoinstaller in /usr/local/lib/python3.11/dist-packages (0.1.0)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install beautifulsoup4 numpy pandas requests selenium matplotlib seaborn regex geckodriver-autoinstaller webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zIhiem08EkvL"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "import urllib.request as urllib2\n",
        "from urllib.error import HTTPError\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from multiprocessing import Pool\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import *\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from selenium.webdriver.common.by import By\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import argparse\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import os\n",
        "import regex as re\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException, ElementNotVisibleException, StaleElementReferenceException\n",
        "from selenium.webdriver.support.ui import Select\n",
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from selenium.webdriver.common.by import By\n",
        "import pandas as pd\n",
        "import geckodriver_autoinstaller\n",
        "from webdriver_manager.chrome import ChromeDriverManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T2VyTD1EkvM"
      },
      "source": [
        "## MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "tQZNWz6HEkvN"
      },
      "outputs": [],
      "source": [
        "pages = ['https://www.fahasa.com/sach-trong-nuoc/van-hoc-trong-nuoc/tieu-thuyet.html'] + \\\n",
        "    [f'https://www.fahasa.com/sach-trong-nuoc/van-hoc-trong-nuoc/tieu-thuyet.html?order=num_orders&limit=24&p={i}' for i in range(2, 87)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7pQ4ihbjEkvN"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "\n",
        "bookTitles = []\n",
        "images = []\n",
        "urls = []\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\"}\n",
        "\n",
        "def take_bookTitles_and_images(page):\n",
        "    r = requests.get(page, headers=headers)\n",
        "    r.raise_for_status()\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "    titles = [img_tag.find('img')['alt'] for img_tag in soup.find_all('span', class_='product-image')]\n",
        "    imgs = [img_tag.find('img')['data-src'] for img_tag in soup.find_all('span', class_='product-image')]\n",
        "    page_urls = [a_tag['href'] for a_tag in soup.find_all('a', class_='product-image')]\n",
        "\n",
        "    return titles, imgs, page_urls\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    results = list(executor.map(take_bookTitles_and_images, pages))\n",
        "\n",
        "for titles, imgs, page_urls in results:\n",
        "    bookTitles.extend(titles)\n",
        "    images.extend(imgs)\n",
        "    urls.extend(page_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfmLZRLNEkvN",
        "outputId": "018783d7-7974-4b17-e25c-f940883a9e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1860\n"
          ]
        }
      ],
      "source": [
        "print(len(urls))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QfcdOvh2EkvO"
      },
      "outputs": [],
      "source": [
        "wantedBooks = len(urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "IcKSBRfDEkvO"
      },
      "outputs": [],
      "source": [
        "prices = []\n",
        "details = []\n",
        "genres = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg5AoqLcEkvO"
      },
      "outputs": [],
      "source": [
        "def extract_text_or_default(soup, selector, default=\"No detail\"):\n",
        "    element = soup.select_one(selector)\n",
        "    return element.text if element else default\n",
        "\n",
        "def extract_genres(soup):\n",
        "    genresRaw = soup.select('ol.breadcrumb a')\n",
        "    return [genre.text for genre in genresRaw] if genresRaw else [\"No genre\"]\n",
        "\n",
        "def take_prices_inf_genre(session, url):\n",
        "    r = session.get(url, headers=headers)\n",
        "    r.raise_for_status()\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "    price = extract_text_or_default(soup, 'span.price', \"No price\")\n",
        "    detail = extract_text_or_default(soup, 'div#desc_content', \"No detail\")\n",
        "    genre = extract_genres(soup)\n",
        "\n",
        "    return price, detail, genre\n",
        "\n",
        "def process_url(session, url):\n",
        "    try:\n",
        "        return take_prices_inf_genre(session, url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing URL {url}: {e}\")\n",
        "        return \"No price\", \"No detail\", [\"No genre\"]\n",
        "\n",
        "with requests.Session() as session:\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        results = list(executor.map(lambda url: process_url(session, url), urls))\n",
        "\n",
        "for price, detail, genre in results:\n",
        "    prices.append(price)\n",
        "    details.append(detail)\n",
        "    genres.append(genre)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BLBfkbiEkvO"
      },
      "outputs": [],
      "source": [
        "print(len(bookTitles))\n",
        "print(len(prices))\n",
        "print(len(details))\n",
        "print(len(images))\n",
        "print(len(genres))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lJGjgBeEkvP"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({\"Ten \": bookTitles,\"Gia \": prices ,\"Thong tin\": details,\"Anh\":images,\"The loai\":genres}).to_csv(\"data_fahasa.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg_iWcWEEkvP"
      },
      "source": [
        "## User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXxF8G0mEkvP"
      },
      "outputs": [],
      "source": [
        "# take the data from the csv file\n",
        "data = pd.read_csv(\"data_fahasa.csv\")\n",
        "data = data.drop(columns=[\"Unnamed: 0\"])\n",
        "data = data.dropna()\n",
        "data = data.reset_index(drop=True)\n",
        "data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR5GJBGJEkvP"
      },
      "outputs": [],
      "source": [
        "bookTitles = data[\"Ten \"]\n",
        "prices = data[\"Gia \"]\n",
        "details = data[\"Thong tin\"]\n",
        "images = data[\"Anh\"]\n",
        "genres = data[\"The loai\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxv02slEkvP"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvNf9wh4EkvP"
      },
      "outputs": [],
      "source": [
        "bookTitlesWithoutToneMark = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdT3A0viEkvQ"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "\n",
        "def no_accent_vietnamese(s):\n",
        "    s = re.sub('[áàảãạăắằẳẵặâấầẩẫậ]', 'a', s)\n",
        "    s = re.sub('[ÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬ]', 'A', s)\n",
        "    s = re.sub('[éèẻẽẹêếềểễệ]', 'e', s)\n",
        "    s = re.sub('[ÉÈẺẼẸÊẾỀỂỄỆ]', 'E', s)\n",
        "    s = re.sub('[óòỏõọôốồổỗộơớờởỡợ]', 'o', s)\n",
        "    s = re.sub('[ÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢ]', 'O', s)\n",
        "    s = re.sub('[íìỉĩị]', 'i', s)\n",
        "    s = re.sub('[ÍÌỈĨỊ]', 'I', s)\n",
        "    s = re.sub('[úùủũụưứừửữự]', 'u', s)\n",
        "    s = re.sub('[ÚÙỦŨỤƯỨỪỬỮỰ]', 'U', s)\n",
        "    s = re.sub('[ýỳỷỹỵ]', 'y', s)\n",
        "    s = re.sub('[ÝỲỶỸỴ]', 'Y', s)\n",
        "    s = re.sub('đ', 'd', s)\n",
        "    s = re.sub('Đ', 'D', s)\n",
        "    return s\n",
        "\n",
        "def separate(title):\n",
        "    return title.split('(')[0].split('-')[0]\n",
        "\n",
        "goodReadsOriginalLink = 'https://www.goodreads.com/'\n",
        "goodReadsOriginalSearchLink = 'https://www.goodreads.com/search?q='\n",
        "goodReadsLinkList = []\n",
        "\n",
        "bookTitlesWithoutToneMark = [separate(bookTitle) for bookTitle in bookTitles]\n",
        "\n",
        "def fetch_goodreads_link(bookTitleWithoutToneMark):\n",
        "    urlForBookTitleInGoodReads = goodReadsOriginalSearchLink + '%20'.join(bookTitleWithoutToneMark.split())\n",
        "    urlForBookTitleInGoodReads = no_accent_vietnamese(urlForBookTitleInGoodReads)\n",
        "    try:\n",
        "        html = urlopen(urlForBookTitleInGoodReads)\n",
        "        bs = BeautifulSoup(html.read(), 'html.parser')\n",
        "        bookTitle = bs.find('a', class_='bookTitle')\n",
        "        return goodReadsOriginalLink + bookTitle['href']\n",
        "    except Exception as e:\n",
        "        return ''\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    goodReadsLinkList = list(executor.map(fetch_goodreads_link, bookTitlesWithoutToneMark))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SISwo3sfdXET"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for link in goodReadsLinkList:\n",
        "  if (link == ''):\n",
        "    count = count + 1\n",
        "\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za26H_jNEkvQ"
      },
      "outputs": [],
      "source": [
        "print(len(goodReadsLinkList))\n",
        "print(goodReadsLinkList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoOFdDguEkvQ"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "import concurrent.futures\n",
        "\n",
        "inf = []\n",
        "rating = []\n",
        "all_genres = []\n",
        "\n",
        "def extract_data(soup, selector, attr=None, default=\"\"):\n",
        "    element = soup.select_one(selector)\n",
        "    if element:\n",
        "        return element[attr] if attr else element.text.strip()\n",
        "    return default\n",
        "\n",
        "def genre_about(url):\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.138 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        rate_dict = {'5': 0, '4': 0, '3': 0, '2': 0, '1': 0}\n",
        "        json_data = soup.find('script', type='application/ld+json')\n",
        "        if json_data:\n",
        "            data = json.loads(json_data.string)\n",
        "            if \"aggregateRating\" in data:\n",
        "                rate_dict = data['aggregateRating']['ratingValue']\n",
        "\n",
        "        desc_text = extract_data(soup, 'div.DetailsLayoutRightParagraph__widthConstrained')\n",
        "\n",
        "        genres = [genre_link.text.strip() for genre_link in soup.select('div[data-testid=\"genresList\"] a')]\n",
        "        genres = list(set(genres)) if genres else []\n",
        "\n",
        "        return desc_text, rate_dict, genres\n",
        "\n",
        "    except Exception as e:\n",
        "        return '', '', []\n",
        "\n",
        "# Sử dụng ThreadPoolExecutor để thực hiện các yêu cầu song song\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    results = list(executor.map(genre_about, goodReadsLinkList))\n",
        "\n",
        "# Tách kết quả ra thành các danh sách riêng biệt\n",
        "for desc_text, rate_dict, genres_list in results:\n",
        "    inf.append(desc_text)\n",
        "    rating.append(rate_dict)\n",
        "    all_genres.append(genres_list)\n",
        "\n",
        "# Thực thi và in kết quả\n",
        "print(inf)\n",
        "print(rating)\n",
        "print(all_genres)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yiddemUEkvQ"
      },
      "outputs": [],
      "source": [
        "dict_id_rating_ratingBook = [{}]\n",
        "options = webdriver.FirefoxOptions()\n",
        "options.headless = True\n",
        "\n",
        "def main(url, book):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.138 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    retries = 3\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            scriptData = soup.find('script', id=\"__NEXT_DATA__\")\n",
        "            for i in json.loads(scriptData.string)['props']['pageProps']['apolloState']:\n",
        "                if (json.loads(scriptData.string)['props']['pageProps']['apolloState'][i]['__typename'] == 'Review'):\n",
        "                    temp = json.loads(scriptData.string)['props']['pageProps']['apolloState'][i]\n",
        "                    dict_id_rating_ratingBook.append({'id': temp['creator']['__ref'].replace('User:', ''), 'bookName': book, 'rating': temp['rating'], 'ratingBook': temp['text']})\n",
        "            break\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            time.sleep(2)\n",
        "        except Exception as e:\n",
        "            break\n",
        "\n",
        "def process_books(urls, books):\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
        "        executor.map(lambda p: main(*p), zip(urls, books))\n",
        "\n",
        "process_books(goodReadsLinkList, bookTitles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G-u7hIaEkvR"
      },
      "outputs": [],
      "source": [
        "user_id = []\n",
        "user_rating = []\n",
        "user_rating_book = []\n",
        "user_review = []\n",
        "for i in range(1,len(dict_id_rating_ratingBook)):\n",
        "    user_id.append(dict_id_rating_ratingBook[i]['id'])\n",
        "    user_rating.append(dict_id_rating_ratingBook[i]['rating'])\n",
        "    user_rating_book.append(dict_id_rating_ratingBook[i]['bookName'])\n",
        "    user_review.append(dict_id_rating_ratingBook[i]['ratingBook'])\n",
        "print(len(user_id))\n",
        "print(len(user_rating))\n",
        "print(len(user_rating_book))\n",
        "print(len(user_review))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb6SHeKjqXJ-"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "book_user_dict = defaultdict(list)\n",
        "for entry in dict_id_rating_ratingBook:\n",
        "    if 'bookName' in entry and 'id' in entry:\n",
        "        book_user_dict[entry['bookName']].append(entry['id'])\n",
        "rating_users_for_each_books = [book_user_dict[book] for book in bookTitles]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(rating_users_for_each_books)"
      ],
      "metadata": {
        "id": "0NmCMXRTzUeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMGfkawHEkvR"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({\"id \": user_id,\"rating \": user_rating ,\"name\": user_rating_book,\"Review\": user_review}).to_csv(\"review_data.csv\")\n",
        "pd.DataFrame({'name':bookTitles,'thong tin':inf,'rating':rating,'genre':all_genres,'rating_users':rating_users_for_each_books}).to_csv(\"good_read.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEsdjJevEkvR"
      },
      "source": [
        "# Gộp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNRekt56EkvR"
      },
      "outputs": [],
      "source": [
        "fhs = pd.read_csv('data_fahasa.csv')\n",
        "gr = pd.read_csv('good_read.csv')\n",
        "rd = pd.read_csv('review_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W39IYdf3hmQi"
      },
      "outputs": [],
      "source": [
        "print(fhs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxdAmMI6hsLC"
      },
      "outputs": [],
      "source": [
        "print(gr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTmPq2nOhv2M"
      },
      "outputs": [],
      "source": [
        "print(rd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJs0Aez4EkvR"
      },
      "outputs": [],
      "source": [
        "def no_accent_vietnamese(s):\n",
        "    s = re.sub('\\xa0', ' ', s)\n",
        "    s = re.sub('[áàảãạăắằẳẵặâấầẩẫậ]', 'a', s)\n",
        "    s = re.sub('[ÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬ]', 'A', s)\n",
        "    s = re.sub('[éèẻẽẹêếềểễệ]', 'e', s)\n",
        "    s = re.sub('[ÉÈẺẼẸÊẾỀỂỄỆ]', 'E', s)\n",
        "    s = re.sub('[óòỏõọôốồổỗộơớờởỡợ]', 'o', s)\n",
        "    s = re.sub('[ÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢ]', 'O', s)\n",
        "    s = re.sub('[íìỉĩị]', 'i', s)\n",
        "    s = re.sub('[ÍÌỈĨỊ]', 'I', s)\n",
        "    s = re.sub('[úùủũụưứừửữự]', 'u', s)\n",
        "    s = re.sub('[ÚÙỦŨỤƯỨỪỬỮỰ]', 'U', s)\n",
        "    s = re.sub('[ýỳỷỹỵ]', 'y', s)\n",
        "    s = re.sub('[ÝỲỶỸỴ]', 'Y', s)\n",
        "    s = re.sub('đ', 'd', s)\n",
        "    s = re.sub('Đ', 'D', s)\n",
        "    return s\n",
        "\n",
        "def twodtooned(m):\n",
        "    return [no_accent_vietnamese(lt) if isinstance(lt, str) else '' for lt in m]\n",
        "\n",
        "list_name = twodtooned(fhs['Ten '])\n",
        "temp = twodtooned(rd['name'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHsNvxWsEkvR"
      },
      "outputs": [],
      "source": [
        "list_inf = []\n",
        "list_genres = []\n",
        "real_name = []\n",
        "rdid = []\n",
        "rdrating = []\n",
        "list_img = []\n",
        "\n",
        "def find_index(theList, item):\n",
        "    for ind, val in enumerate(theList):\n",
        "        if item in val:\n",
        "            return ind\n",
        "    return 'Not Found'\n",
        "\n",
        "def append_data(res, i):\n",
        "    if res != 'Not Found':\n",
        "        list_img.append(fhs['Anh'][res])\n",
        "        list_inf.append(fhs['Thong tin'][res])\n",
        "        list_genres.append(gr['genre'][res])\n",
        "        real_name.append(fhs['Ten '][res])\n",
        "    else:\n",
        "        list_inf.append('')\n",
        "        list_genres.append('')\n",
        "        list_img.append('')\n",
        "        real_name.append('')\n",
        "\n",
        "for i, item in enumerate(temp):\n",
        "    res = find_index(list_name, item)\n",
        "    rdid.append(rd['id '][i])\n",
        "    rdrating.append(rd['rating '][i])\n",
        "    append_data(res, i)\n",
        "\n",
        "st = gr['rating_users']\n",
        "for i in range(len(st)):\n",
        "    if isinstance(st[i], float):\n",
        "        list_img.append(fhs['Anh'][i])\n",
        "        real_name.append(gr['name'][i])\n",
        "        list_inf.append(gr['Thong tin'][i])\n",
        "        list_genres.append(gr['genre'][i])\n",
        "        rdid.append('')\n",
        "        rdrating.append('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmGGXuPwEkvR"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'id':rdid,'rating':rdrating,'name':real_name,'Hinh':list_img,'Thong tin':list_inf,'The Loai':list_genres}).to_csv('All.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD8QKVvhEkvS"
      },
      "outputs": [],
      "source": [
        "fileCSV = pd.read_csv(\"All.csv\")\n",
        "pd.DataFrame({'User-ID':fileCSV['id']}).to_csv('Users.csv')\n",
        "pd.DataFrame({'User-ID':fileCSV['id'],'Book-Rating':fileCSV['rating']}).to_csv('Ratings.csv')\n",
        "pd.DataFrame({'Book-Title':fileCSV['name'],'Image':fileCSV['Hinh'],'Description':fileCSV['Thong tin'],'Genres':fileCSV['The Loai']}).to_csv('Books.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "books = pd.read_csv('Books.csv')\n",
        "ratings = pd.read_csv('Ratings.csv')\n",
        "users = pd.read_csv('Users.csv')"
      ],
      "metadata": {
        "id": "tXpL1uYJ1g1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books"
      ],
      "metadata": {
        "id": "LRdonG231sCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings"
      ],
      "metadata": {
        "id": "IfHsvy8u1yDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users"
      ],
      "metadata": {
        "id": "Ifi9_JJz10Zs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}